{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waldyr Faustini - FE630 - Assignment 2\n",
    "\n",
    "## Prof. Papa Momar Ndiaye - FE630 Portfolio Theory and Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Eingenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the matrix **A**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 1 & 1 \\\\ 1 & 1 & 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So let's starting calculating the **Characteristic Polynomial of A**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\lambda)= det (A- \\lambda I)$$\n",
    "\n",
    "$$p(\\lambda)= det\\begin{bmatrix} 1-\\lambda & 2 & 1 \\\\ 2 & 1-\\lambda & 1 \\\\ 1 & 1 & 2-\\lambda \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\lambda) = +[(1-\\lambda).(1-\\lambda).(2-\\lambda)+(2.1.1)+(1.2.1)]-[(1.(1-\\lambda).1+1.(1-\\lambda)+2.2.(2-\\lambda)]$$\n",
    "\n",
    "$$p(\\lambda) = -(\\lambda-4).(\\lambda+1).(\\lambda-1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, rearranging the terms we have:\n",
    "\n",
    "$$\\boxed { p(\\lambda) = -\\lambda^3 + 4\\lambda^2 + \\lambda - 4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can calculate the **Eingevalues**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the equation of Characteristic Polynomial of A, we have: $p(\\lambda) = -(\\lambda-4).(\\lambda+1).(\\lambda-1)$ or $p(\\lambda) = -\\lambda^3 + 4\\lambda^2 + \\lambda - 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find the $\\lambda$'s to turn $p(\\lambda) = 0$. So, using :$p(\\lambda) = -(\\lambda-4).(\\lambda+1).(\\lambda-1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily identify $\\boxed {\\lambda_{1} = 4}$, $\\boxed {\\lambda_{2} = -1}$ and $\\boxed {\\lambda_{3} = +1}$ as eingevalues for this $p(\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For each **Eingevalue** we have one **Eingevector**. So we have 3 Eingevectors here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda_{1} = 4$  we have :\n",
    "  $$\\vec{p_{1}} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda_{2} = -1$  we have :\n",
    "  $$\\vec{p_{2}} = \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\lambda_{3} = -1$  we have :\n",
    "  $$\\vec{p_{3}} = \\begin{bmatrix} -1 \\\\ -1 \\\\ 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the 3 Eingevectors are:\n",
    "\n",
    "$$\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} , \\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix} , \\begin{bmatrix} -1 \\\\ -1 \\\\ 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the 3 eingevectors are orthogonal, it's a quick check just calculating the scalar product between them:\n",
    "\n",
    "$$\\vec p_{1} \\cdot \\vec p_{3} = 1.(-1) + 1.(-1) + 1.2 = -1 -1 +2 = 0$$\n",
    "\n",
    "$$\\vec p_{1} \\cdot \\vec p_{2} = 1.(-1) + 1.(1) + 1.0 = -1 +1 +0 = 0$$\n",
    "\n",
    "$$\\vec p_{3} \\cdot \\vec p_{2} = (-1).(-1) + (-1).1 + 2.0 = +1 -1 +0 = 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we are sure all the vectors are orthogonal, but we need the vectors be orthonormal. So, each vector need to lenght 1.\n",
    "So, we first need to calculate all the norms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$|\\vec p_{1}| = \\sqrt{1^2 +1^2+1^2} = \\sqrt{3}$\n",
    "\n",
    "$|\\vec p_{2}| = \\sqrt{(-1)^2 +1^2+0^2} = \\sqrt{2}$\n",
    "\n",
    "$|\\vec p_{3}| = \\sqrt{(-1)^2 +(-1)^2+2^2} = \\sqrt{6}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we multiplicate each vector by $\\frac{1}{|vector|}$, we will normalize $|\\vec p_{1}|, |\\vec p_{2}|$  and  $|\\vec p_{3}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizing $\\vec{p_{1}}$ we have to multiply $\\frac{1}{\\sqrt{3}}\\cdot\\vec{p_{1}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{\\sqrt{3}}\\cdot\\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\\\ \\frac{1}{\\sqrt{3}} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizing $\\vec{p_{2}}$ we have to multiply $\\frac{1}{\\sqrt{2}}\\cdot\\vec{p_{2}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{\\sqrt{2}}\\cdot\\begin{bmatrix} -1 \\\\ 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} \\frac{-1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Normalizing $\\vec{p_{3}}$ we have to multiply $\\frac{1}{\\sqrt{6}}\\cdot\\vec{p_{3}}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{1}{\\sqrt{6}}\\cdot\\begin{bmatrix} -1 \\\\ -1 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{-1}{\\sqrt{6}} \\\\ \\frac{-1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{6}} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can have the matrix ${P}$ orthornormalized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${P} = \\begin{bmatrix} \\frac{1}{\\sqrt{3}} & \\frac{-1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} & \\frac{1}{\\sqrt{2}} & \\frac{-1}{\\sqrt{6}} \\\\ \\frac{1}{\\sqrt{3}} & 0  & \\frac{2}{\\sqrt{6}} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the Eingevalues, so it's easy to get the diagonal matrix D. Each eingevalue will be an element of the diagonal, the other elements will be zero. So, the diaginal matrix D is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ {D} = \\begin{bmatrix} 4 & 0 & 0 \\\\ 0 & -1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the matrix $P^{-1}$, i used the normal process to write an identity matrix 3x3 on the right and then doing some algebric operations between line and columns we can move this identity matrix to the left. So, the final result for $P^{-1}$ is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${P^{-1}} = \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & \\frac{\\sqrt{3}}{2} & 0 \\\\ \\frac{-1}{2\\sqrt{2}} & \\frac{3}{2\\sqrt{2}} & \\frac{-1}{\\sqrt{2}} \\\\ \\frac{-\\sqrt{3}}{2\\sqrt{2}} & \\frac{-\\sqrt{3}}{2\\sqrt{2}}  & \\frac{\\sqrt{3}}{\\sqrt{2}} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the matrix A is invertible. The proof is, we found matrix's (previous exercise) to fill the conditions:\n",
    "$$ A = PDP^{-1}$$\n",
    "so we have the matrix P with linear independt vectors (Eingevectors of A) and the diagonal matrix D (Eingevalues of A). So it is suficient to affirm **A is invertible**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we have to write $A^{-1}$ in terms of **P** and **D**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ A = PDP^{-1} \\rightarrow (A)^{-1} = (PDP^{-1})^{-1} $$ \n",
    "\n",
    "$$\\boxed{ A^{-1} = PD^{-1}P^{-1} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just applied the the exponent **-1** in both sides of the equation. So we have the expression for $A^{-1}$ in terms of **P** and **D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$${A^{-1}} = \\begin{bmatrix} \\frac{-1}{4} & \\frac{3}{4} & \\frac{-1}{4} \\\\ \\frac{3}{4} & \\frac{-1}{4} & \\frac{-1}{4} \\\\ \\frac{-1}{4} & \\frac{-1}{4}  & \\frac{3}{4} \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So let's starting calculating the **Characteristic Polynomial of $A^{-1}$**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\lambda)= det (A^{-1}- \\lambda I)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\lambda)= det \\begin{bmatrix} \\frac{-1}{4}- \\lambda & \\frac{3}{4} & \\frac{-1}{4} \\\\ \\frac{3}{4} & \\frac{-1}{4} - \\lambda & \\frac{-1}{4} \\\\ \\frac{-1}{4} & \\frac{-1}{4}  & \\frac{3}{4}- \\lambda \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed { p(\\lambda) = \\frac{-4\\lambda^3 + \\lambda^2 + 4\\lambda - 1}{4}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we can calculate the **Eingevalues**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to find the $\\lambda$'s to turn $p(\\lambda) = 0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily identify $\\boxed {\\lambda_{1} = \\frac{1}{4}}$, $\\boxed {\\lambda_{2} = 1}$ and $\\boxed {\\lambda_{3} = -1}$ are eingevalues for this $p(\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define matrix A using Numpy arrays\n",
    "A = np.array([[1, 2, 1],\n",
    " [2, 1, 1],\n",
    " [1, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. -1.  1.]\n"
     ]
    }
   ],
   "source": [
    "from numpy import linalg as la\n",
    "\n",
    "eig_val, eig_vect = la.eig(A)\n",
    "\n",
    "# Now we can find the Eingevalues:\n",
    "print(eig_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.77350269e-01 -7.07106781e-01 -4.08248290e-01]\n",
      " [-5.77350269e-01  7.07106781e-01 -4.08248290e-01]\n",
      " [-5.77350269e-01  1.02381169e-16  8.16496581e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can find the Orthonormalized Eingevectors:\n",
    "\n",
    "print(eig_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25  0.75 -0.25]\n",
      " [ 0.75 -0.25 -0.25]\n",
      " [-0.25 -0.25  0.75]]\n"
     ]
    }
   ],
   "source": [
    "# Now we can find the Inverse:\n",
    "\n",
    "inverse_A = la.inv(A)\n",
    "\n",
    "print(inverse_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Non Constrained Optimization and Convexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. and 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to check if the function $f$ is convex:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed {f : (x_{1},x_{2},x_{3}) \\rightarrow f(x_{1},x_{2},x_{3}) = x_{1}^{4}+(x_{1}+x_{2})^{2}+(x_{1}+x_{3})^{2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets analyze the gradient and see the extrema points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(\\left.x_{1}, x_{2}, x_{3}\\right)=\\left[\\begin{array}{c}\n",
    "\\dfrac{\\partial f}{\\partial x_1}(\\left.x_{1}, x_{2}, x_{3}\\right)\\\\\n",
    "\\dfrac{\\partial f}{\\partial x_2}(\\left.x_{1}, x_{2}, x_{3}\\right) \\\\\n",
    "\\dfrac{\\partial f}{\\partial x_3}(\\left.x_{1}, x_{2},  x_{3}\\right) \n",
    "\\end{array}\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla f(\\left.x_{1}, x_{2}, x_{3}\\right)=\n",
    "\\begin{bmatrix} 4x_{1}^3+4x_1+2x_{2}+2x_{3}\\\\ 2x_{1}+2x_{2} \\\\ 2x_{1}+2x_{3} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the extrema point, we need to assume $\\nabla f(\\left.x_{1}, x_{2}, x_{3}\\right)= 0$. So:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) $ 4x_{1}^3+4x_1+2x_{2}+2x_{3} = 0$\n",
    "\n",
    "(ii) $ 2x_{1}+2x_{2} = 0 $\n",
    "\n",
    "(iii) $ 2x_{1}+2x_{3} = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **(i)** and **(ii)**:\n",
    "\n",
    "$x_{1} = - x_{2}$\n",
    "\n",
    "$x_{1} = - x_{3}$\n",
    "\n",
    "Then : $x_{2} = x_{3}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing (i), (ii) and (iii) we can easily see the only point satisfying is : $\\boxed{f(0,0,0)=0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyze the function convexity, we have to look the **Hessian matrix** of the function $f$. The **Hessian matrix** is defined as below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Hessian = \\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1\\partial x_2 } &\n",
    "\\frac{\\partial^2 f}{\\partial x_1\\partial x_3 }\n",
    "\\\\ \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} &\n",
    "\\frac{\\partial^2 f}{\\partial x_2\\partial x_3 } \n",
    "\\\\ \\frac{\\partial^2 f}{\\partial x_3\\partial x_1} & \\frac{\\partial^2 f}{\\partial x_3\\partial x_2} &\n",
    "\\frac{\\partial^2 f}{\\partial x_3^2} \n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the partial derivatives, we have as result the matrix below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Hessian = \\begin{bmatrix} 12x_{1}^{2}+4 & 2 & 2 \\\\ 2 & 2 & 0 \\\\ 2 & 0 & 2 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ det Hessian = 48x_1^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant will be non negative for any number, but could be zero for $x_1 = 0$, so the function is **Convex semi-definite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the determinants, analyzing the minors of the Hessian matrix above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ det A_1 = det \\begin{bmatrix} 12x_1^{2} +4 \\end{bmatrix} = 12x_1^{2} +4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ det A_2 = det \\begin{bmatrix} 12x_1^{2}  +4 & 2 \\\\ 2 & 2 \\end{bmatrix} = 24x_1^{2} +4 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ det A_3 = det \\begin{bmatrix} 12x_1^{2}  +4 & 2 &2 \\\\ 2 & 2 & 0 \\\\ 2 & 0 & 2 \\end{bmatrix} = 48x_1^{2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the extrema is (0,0,0), so we can substitute in the results of $detA_1$, $detA_2$ and $detA_3$:\n",
    "\n",
    "$detA_1 = 4$\n",
    "\n",
    "$detA_2 = 4$\n",
    "\n",
    "$detA_3 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, these determinants ($detA_1, detA_2, detA_3$)  are all nonnegative then the **matrix is positive semidefinite**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extrema points **(0,0,0)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the file \"Class_Participation_1.ipynb\" available in Module 4. Adaptating the code to use as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "f = lambda x: x[0]**2 + (x[0]+x[1])**2 + (x[0]+x[2])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we don't have constraints in this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fun: 0.0\n",
      " hess_inv: array([[1, 0, 0],\n",
      "       [0, 1, 0],\n",
      "       [0, 0, 1]])\n",
      "      jac: array([4.47034836e-08, 1.49011612e-08, 1.49011612e-08])\n",
      "  message: 'Optimization terminated successfully.'\n",
      "     nfev: 4\n",
      "      nit: 0\n",
      "     njev: 1\n",
      "   status: 0\n",
      "  success: True\n",
      "        x: array([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#extrema point\n",
    "\n",
    "x_0 = np.array([0,0,0])\n",
    "\n",
    "\n",
    "res = minimize(f,x_0,method = 'BFGS')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Optimization with Equality Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to min $x_1,x_2,x_3$:\n",
    "\n",
    "$$\\boxed {f(x_1,x_2,x_3) = x_1 + x_2 + 2x_3^{2}}$$\n",
    "\n",
    "Using the constraints:\n",
    "\n",
    "\n",
    "$$\\boxed {x_1 = 1 \\rightarrow h_1(x_1) = x_1-1=0}$$\n",
    "$$\\boxed {x_1^{2}+x_2^{2} = 1 \\rightarrow h_2(x_1,x_2) = x_1^{2}+x_2^{2} -1=0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's first analyze the constraints, $h_1(x_1)$ and $h_2(x_1,x_2)$, to check the first equation: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From $h_1(x_1)$ it's pretty easy to identify $\\boxed{x_1=1}$. Now let's use this result in $h_2(x_1,x_2)$:\n",
    "\n",
    "$1^2+x_2^{2}=1 \\rightarrow x_2^{2} = 0 \\rightarrow \\boxed {x_2 = 0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the 2 out of 3 points to minimize this function using this problem constraints, in this case **(1,0,$x_3$)**. Now lets substitute in the function $f(x_1,x_2,x_3)$ this points:\n",
    "\n",
    "$f(1,0,x_3) = 1 + 0 +2x_3^{2} \\rightarrow f(1,0,x_3) = 1+2x_3^{2}$\n",
    "\n",
    "Now we have to calculate the gradient to see the minimum point for this function:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial x_3} = 4x_3 \\rightarrow 4x_3 = 0 \\rightarrow x_3 = 0$\n",
    "\n",
    "So the minimum point for this function is $\\boxed {(1,0,0)}$\n",
    "\n",
    "$\\boxed {f(1,0,0) = 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use the Lagrangian approach:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(x_1,x_2,x_3,\\lambda_1,\\lambda_2) = f(x_1,x_2,x_3)+\\lambda_1(h(x_1))+ \\lambda_2(h(x_1,x_2))$\n",
    "\n",
    "$L(x_1,x_2,x_3,\\lambda_1,\\lambda_2) = x_1 + x_2 + 2x_3^{2}+\\lambda_1(x_1-1)+\\lambda_2(x_1^{2}+x_2^{2}-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should calculate the gradient of $L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla L(\\left.x_{1}, x_{2}, x_{3},\\lambda_1,\\lambda_2\\right)=(1+\\lambda_1+2\\lambda_2x_1,1+2\\lambda_2x_2,4x_3,x_1-1,(x_1^{2}+x_2^{2}-1))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the min of the function : $\\nabla L(\\left.x_{1}, x_{2}, x_{3},\\lambda_1,\\lambda_2\\right)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed{4x_3 = 0 \\rightarrow x_3 = 0}$\n",
    "\n",
    "$\\boxed{x_1-1 = 0 \\rightarrow x_1 = 1}$\n",
    "\n",
    "$\\boxed{(x_1^{2}+x_2^{2}-1) = 0 \\rightarrow x_2 = 0}$\n",
    "\n",
    "for $\\forall \\lambda_1, \\lambda_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(x_1,x_2,x_3) \\rightarrow (1,0,0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is not unique for $\\lambda_1,\\lambda_2$, so **we cannot use Lagrangian approach in this case**, because we cannot use an unique value for $\\lambda_1,\\lambda_2$ to satisfy the minimum value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the file \"Class_Participation_1.ipynb\" available in Module 4. Adaptating the code to use as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "f = lambda x: x[0]+x[1]+2*x[2]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constraints\n",
    "\n",
    "con = ({'type': 'eq','fun': lambda x: np.array(x[0]-1)},\n",
    "        {'type': 'eq','fun': lambda x: np.array(x[0]**2 + x[1]**2 - 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 1.0\n",
      "     jac: array([1.00000000e+00, 1.00000000e+00, 2.98023224e-08])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 4\n",
      "     nit: 1\n",
      "    njev: 1\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "#(x1,x2,x3) -> (1,0,0)\n",
    "x_0 = [1,0,0]\n",
    "\n",
    "b = (-10,10) \n",
    "bounds = (b,b,b)\n",
    "\n",
    "res = minimize(f,x_0,method = 'SLSQP',bounds=bounds,constraints=con)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to min $x_1,x_2$:\n",
    "\n",
    "$$\\boxed {f(x_1,x_2) = 2x_1^{2} + x_2^{2}}$$\n",
    "\n",
    "Using the constraints:\n",
    "\n",
    "\n",
    "$$\\boxed {x_1 + x_2 = 1 \\rightarrow h(x_1,x_2) = x_1+x_2-1=0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to use the Lagrangian approach:\n",
    "\n",
    "$L(x_1,x_2\\lambda_1,) = f(x_1,x_2,x_3)+\\lambda(h(x_1))$\n",
    "\n",
    "The Lagrangian is:\n",
    "\n",
    "$$\\boxed{L(x_1,x_2,\\lambda) = 2x_1^{2} + x_2^{2} + \\lambda(x_1+x_2-1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to get the gradient of L and then make equal to zero to find the minimum\n",
    "\n",
    "$\\nabla L(\\left.x_{1}, x_{2},\\lambda\\right)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla L(\\left.x_{1}, x_{2},\\lambda\\right)=(4x_1-\\lambda,2x_2-\\lambda,1-x_1-x_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed {x_1 = \\frac{1}{3}, x_2 = \\frac{2}{3}, \\lambda = \\frac{4}{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the minimum point of the Lagrangian using this coordinates above is:\n",
    "\n",
    "$L\\left(\\frac{1}{3},\\frac{2}{3},\\frac{4}{3}\\right) = 2\\left(\\frac{1}{3}\\right)^2+\\left(\\frac{2}{3}\\right)^2+\\frac{4}{3}\\left(\\frac{1}{3}+\\frac{2}{3}-1\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed{L\\left(\\frac{1}{3},\\frac{2}{3},\\frac{4}{3}\\right) = \\frac{2}{9} + \\frac{4}{9} = \\frac{6}{9} = \\frac{2}{3}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed {f\\left(\\frac{1}{3},\\frac{2}{3}\\right) = \\frac{2}{3} \\sim 0.67}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's change change the constraint.**\n",
    "\n",
    "$$\\boxed {x_1 + x_2 = 1.05 \\rightarrow h(x_1,x_2) = x_1+x_2-1.05=0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the new Lagrangian is:\n",
    "\n",
    "$$\\boxed{L(x_1,x_2,\\lambda) = 2x_1^{2} + x_2^{2} + \\lambda(x_1+x_2-1.05)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to get the gradient of L and then make equal to zero to find the minimum\n",
    "\n",
    "$\\nabla L(\\left.x_{1}, x_{2},\\lambda\\right)=0$\n",
    "\n",
    "$\\nabla L(\\left.x_{1}, x_{2},\\lambda\\right)=(4x_1-\\lambda,2x_2-\\lambda,1.05-x_1-x_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed {x_1 = 0.35, x_2 = 0.7, \\lambda = -1.4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\boxed {f\\left(0.35,0.7\\right) = 2(0.35)^2+(0.7)^2 = 0.735}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if the constraint changes from 1 to 1.05, the minimum changes from $\\sim$0.67 to 0.735"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the file \"Class_Participation_1.ipynb\" available in Module 4. Adaptating the code to use as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "f = lambda x: 2 * x[0]**2 +x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constraints\n",
    "\n",
    "con = ({'type': 'eq','fun': lambda x: np.array(x[0]+x[1]-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.6666666666666667\n",
      "     jac: array([1.33333337, 1.33333336])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 10\n",
      "     nit: 3\n",
      "    njev: 3\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.33333333, 0.66666667])\n"
     ]
    }
   ],
   "source": [
    "#(x1,x2) -> (0.35,0.70)\n",
    "x_0 = [0.35,0.70]\n",
    "\n",
    "b = (-10,10) \n",
    "bounds = (b,b)\n",
    "\n",
    "res = minimize(f,x_0,method = 'SLSQP',bounds=bounds,constraints=con)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to max $\\omega_1,\\omega_2$:\n",
    "\n",
    "$$\\boxed {R_P(\\omega_1,\\omega_2) = \\rho_1\\omega_1 + \\rho_2\\omega_2}$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "\n",
    "$$\\boxed {\\sqrt{ \\sigma_1^2\\omega_1^2 + \\sigma_2^2\\omega_2^2 + 2\\rho_12\\sigma_1\\sigma_2\\omega_1\\omega_2} = \\sigma_T \\rightarrow h_1(\\omega_1,\\omega_2) = \\sqrt{ \\sigma_1^2\\omega_1^2 + \\sigma_2^2\\omega_2^2 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_1\\omega_2} - \\sigma_T } $$\n",
    "\n",
    "$$\\boxed {\\omega_1 + \\omega_2 = 1 \\rightarrow h_2(\\omega_1,\\omega_2) =\\omega_1 + \\omega_2 -1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the Lagrangian:\n",
    "\n",
    "$$L(\\omega_1,\\omega_2,\\lambda_1,\\lambda_2) = R_P(\\omega_1,\\omega_2) + \\lambda_1 h_1(\\omega_1,\\omega_2)+ \\lambda_2 h_2(\\omega_1,\\omega_2) $$\n",
    "\n",
    "$$L(\\omega_1,\\omega_2,\\lambda_1,\\lambda_2) = (\\rho_1\\omega_1 + \\rho_2\\omega_2) + \\lambda_1\\left(\\sqrt{ \\sigma_1^2\\omega_1^2 + \\sigma_2^2\\omega_2^2 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_1\\omega_2} - \\sigma_T\\right)+\\lambda_2(\\omega_1 + \\omega_2 -1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i)** $\\frac{\\partial L}{\\partial \\omega_1} = \\rho_1 + \\frac{\\lambda_1}{2} \\frac{2\\sigma_1^2\\omega_1 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_2}{\\sqrt{ \\sigma_1^2\\omega_1^2 + \\sigma_2^2\\omega_2^2 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_1\\omega_2}} + \\lambda_2 = 0 $\n",
    "\n",
    "**(ii)** $\\frac{\\partial L}{\\partial \\omega_2} = \\rho_2 + \\frac{\\lambda_1}{2} \\frac{2\\sigma_2^2\\omega_2 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_1}{\\sqrt{ \\sigma_1^2\\omega_1^2 + \\sigma_2^2\\omega_2^2 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_1\\omega_2}} + \\lambda_2 = 0 $\n",
    "\n",
    "**(iii)** $\\frac{\\partial L}{\\partial \\lambda_1} = \\sqrt{ \\sigma_1^2\\omega_1^2 + \\sigma_2^2\\omega_2^2 + 2\\rho_{12}\\sigma_1\\sigma_2\\omega_1\\omega_2} - \\sigma_T = 0 $\n",
    "\n",
    "**(iv)** $\\frac{\\partial L}{\\partial \\lambda_2} = \\omega_1 + \\omega_2 -1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Message to Professor Papa** :  Professor I tried to solve this problem, but I got stuck in how to find the formula for $\\omega_1$ and $\\omega_2$ , unfortunately was impossible to solve the other questions without this result. I watched the classes and looked in the books but unfortunately I couldn't get the solution for this problem. \n",
    "This situation was a bit complicated, frustrating because I didn't solve the problem, but I tried until the last minutes of Sunday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Optimization with Inequality Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to min $x_1,x_2$:\n",
    "\n",
    "$$\\boxed {f(x_1,x_2) = (x_1-2)^{2} + 2(x_2-1)^{2}}$$\n",
    "\n",
    "Using the constraints:\n",
    "\n",
    "\n",
    "(i)$\\boxed {x_1 + 4x_2 \\leqslant 3}$\n",
    "\n",
    "(ii)$\\boxed {x_1 \\geqslant x_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easier to rearrange **(ii)**:\n",
    "\n",
    "$\\boxed {x_1-x_2 \\geqslant 0 \\rightarrow x_2-x_1 \\leqslant 0}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the Lagrangian for this problem:\n",
    "\n",
    "$\\boxed{L(x_1,x_2,\\lambda_1, \\lambda_2) = (x_1-2)^{2} + 2(x_2-1)^{2}+\\lambda_1(x_1+4x_2-3)+\\lambda_2(x_2-x_1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the Lagrangian, now we can calculate the gradient:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_1}= 2x_1- 4 - \\lambda_1 - \\lambda_2$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_2} = 4x_2-4+4\\lambda_1+\\lambda_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_1(x_1+4x_2-3)=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\lambda_2(x_2-x_1) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have 4 cases to analyze. Analyzing all the cases we get:\n",
    "\n",
    "$\\boxed{x_1 = \\frac{3}{5}}$\n",
    "\n",
    "$\\boxed{x_2 = \\frac{3}{5}}$\n",
    "\n",
    "$\\boxed{f(x_1,x_2) = \\frac{53}{25}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Python now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the file \"Class_Participation_1.ipynb\" available in Module 4. Adaptating the code to use as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "f = lambda x: (x[0]-2)**2+2*((x[1]-1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constraints\n",
    "\n",
    "con = ({'type': 'ineq','fun': lambda x: np.array(3 - x[0] - 4 * x[1])},\n",
    "        {'type': 'ineq','fun': lambda x: np.array(x[1] - x[0])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 2.2799999999998986\n",
      "     jac: array([-2.79999998, -1.59999996])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 6\n",
      "     nit: 2\n",
      "    njev: 2\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0.6, 0.6])\n"
     ]
    }
   ],
   "source": [
    "#(x1,x2) -> (0,0)\n",
    "x_0 = [0,0]\n",
    "\n",
    "b = (-10,10) \n",
    "bounds = (b,b)\n",
    "\n",
    "res = minimize(f,x_0,method = 'SLSQP',bounds=bounds,constraints=con)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results in the simulation are different than I get by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to max $x_1,x_2$:\n",
    "\n",
    "$$\\boxed {f(x_1,x_2) = 5 - x_1^{2} - x_1x_2 - 3x_2^{2}}$$\n",
    "\n",
    "Using the constraints:\n",
    "\n",
    "\n",
    "$$\\boxed {x_1,x_2\\geqslant 0 }$$\n",
    "$$\\boxed {x_1x_2 \\geqslant 2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have the gradients:\n",
    "\n",
    "$\\nabla f(x_1,x_2) = (-2x_1-x_2,-x_1-6x_2) $\n",
    "\n",
    "$\\nabla h_1(x_1,x_2) =(1,0) $\n",
    "\n",
    "$\\nabla h_2(x_1,x_2) =(0,1) $\n",
    "\n",
    "$\\nabla h_3(x_1,x_2) = (x_2,x_1) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write the Lagrangian for this problem:\n",
    "\n",
    "$\\boxed{L(x_1,x_2,\\lambda_1, \\lambda_2) = (5 - x_1^{2} - x_1x_2 - 3x_2^{2})+\\lambda_1(x_1x_2-2)+\\lambda_2x_1+\\lambda_3x_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L}{\\partial x_1}= -2x_1- x_2+\\lambda_1x_2+\\lambda_2 $\n",
    "\n",
    "$\\frac{\\partial L}{\\partial x_2} = -x_1-6x_2+\\lambda_1x_1+\\lambda_3$\n",
    "\n",
    "$\\lambda_1(x_1x_2-2)=0$\n",
    "\n",
    "$\\lambda_2x_1 = 0$\n",
    "\n",
    "$\\lambda_3x_2 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing case 1 and case 2:\n",
    "\n",
    "$\\boxed{x_1 = 1.8170}$\n",
    "\n",
    "$\\boxed{x_2 = 1.10}$\n",
    "\n",
    "$\\boxed{f(x_1,x_2) = 3.9370}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with Python now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the file \"Class_Participation_1.ipynb\" available in Module 4. Adaptating the code to use as optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function\n",
    "\n",
    "f = lambda x: x[0]**2+x[0]*x[1]+3*(x[1]**2)-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constraints\n",
    "\n",
    "con = ({'type': 'ineq','fun': lambda x: np.array(x[0] *  x[1] - 2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 3.928203223666145\n",
      "     jac: array([4.79698992, 8.30862856])\n",
      " message: 'Optimization terminated successfully'\n",
      "    nfev: 302\n",
      "     nit: 78\n",
      "    njev: 76\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([1.86121003, 1.07456975])\n"
     ]
    }
   ],
   "source": [
    "#(x1,x2) -> (1,2)\n",
    "x_0 = [1,2]\n",
    "\n",
    "b = (-10,10) \n",
    "bounds = (b,b)\n",
    "\n",
    "res = minimize(f,x_0,method = 'SLSQP',bounds=bounds,constraints=con)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
